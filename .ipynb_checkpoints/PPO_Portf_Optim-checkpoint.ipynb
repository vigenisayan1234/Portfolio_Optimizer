{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90677601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Portfolio environment with transaction costs and liquidity risk\n",
    "class PortfolioEnvRealTime:\n",
    "    def __init__(self, asset_symbols, initial_cash=100000, max_assets=10, \n",
    "                 transaction_cost=0.001, liquidity_penalty=0.1, api_key=\"YOUR_ALPHA_VANTAGE_API_KEY\"):\n",
    "        self.asset_symbols = asset_symbols\n",
    "        self.cash = initial_cash\n",
    "        self.max_assets = max_assets\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.liquidity_penalty = liquidity_penalty\n",
    "        self.portfolio = {}\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.api_key = api_key\n",
    "        self.asset_prices = {symbol: None for symbol in asset_symbols}\n",
    "        self.total_value = initial_cash\n",
    "\n",
    "    def fetch_real_time_data(self):\n",
    "        \"\"\"Fetches real-time data for each asset.\"\"\"\n",
    "        base_url = \"https://www.alphavantage.co/query\"\n",
    "        for symbol in self.asset_symbols:\n",
    "            params = {\n",
    "                \"function\": \"TIME_SERIES_INTRADAY\",\n",
    "                \"symbol\": symbol,\n",
    "                \"interval\": \"1min\",\n",
    "                \"apikey\": self.api_key\n",
    "            }\n",
    "            response = requests.get(base_url, params=params)\n",
    "            data = response.json()\n",
    "            if 'Time Series (1min)' in data:\n",
    "                last_refreshed = list(data['Time Series (1min)'].keys())[0]\n",
    "                self.asset_prices[symbol] = float(data['Time Series (1min)'][last_refreshed]['4. close'])\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for {symbol}: {data}\")\n",
    "                self.asset_prices[symbol] = None\n",
    "            time.sleep(12)\n",
    "\n",
    "    def calculate_portfolio_value(self):\n",
    "        return self.cash + sum(self.portfolio.get(symbol, 0) * self.asset_prices[symbol] \n",
    "                               for symbol in self.asset_symbols if self.asset_prices[symbol] is not None)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.fetch_real_time_data()\n",
    "        transaction_cost = 0\n",
    "        liquidity_penalty = 0\n",
    "\n",
    "        # Simulate trades based on action\n",
    "        for i, symbol in enumerate(self.asset_symbols):\n",
    "            if action[i] > 0:\n",
    "                asset_price = self.asset_prices.get(symbol, None)\n",
    "                if asset_price:\n",
    "                    trade_volume = action[i] * self.cash\n",
    "                    transaction_cost += trade_volume * self.transaction_cost\n",
    "                    liquidity_penalty += trade_volume * self.liquidity_penalty\n",
    "                    self.portfolio[symbol] = self.portfolio.get(symbol, 0) + (trade_volume / asset_price)\n",
    "                    self.cash -= trade_volume\n",
    "\n",
    "        portfolio_value = self.calculate_portfolio_value()\n",
    "        reward = (portfolio_value - self.total_value) - (transaction_cost + liquidity_penalty)\n",
    "        self.total_value = portfolio_value\n",
    "        state = [self.asset_prices[symbol] for symbol in self.asset_symbols] + [self.cash, portfolio_value]\n",
    "        self.done = self.current_step >= 1000\n",
    "\n",
    "        return state, reward, self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.cash = 100000\n",
    "        self.portfolio = {}\n",
    "        self.current_step = 0\n",
    "        self.fetch_real_time_data()\n",
    "        return [self.asset_prices[symbol] for symbol in self.asset_symbols] + [self.cash, self.total_value]\n",
    "\n",
    "# PPO Policy and Value networks\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, output_dim, lr=0.001, gamma=0.99, epsilon=0.2):\n",
    "        self.policy_net = PolicyNetwork(input_dim, output_dim)\n",
    "        self.value_net = ValueNetwork(input_dim)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        action_probs = self.policy_net(state_tensor)\n",
    "        action = torch.distributions.Categorical(action_probs).sample()\n",
    "        return action.item(), action_probs[action]\n",
    "\n",
    "    def compute_returns(self, rewards, dones, last_value):\n",
    "        returns = []\n",
    "        R = last_value\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "            R = r + self.gamma * R * (1 - d)\n",
    "            returns.insert(0, R)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def update(self, states, actions, rewards, dones, old_action_probs):\n",
    "        returns = self.compute_returns(rewards, dones, 0)\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        actions_tensor = torch.LongTensor(actions)\n",
    "        old_action_probs_tensor = torch.FloatTensor(old_action_probs)\n",
    "\n",
    "        for _ in range(5):  # Multiple epochs\n",
    "            action_probs = self.policy_net(states_tensor).gather(1, actions_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "            ratio = action_probs / old_action_probs_tensor\n",
    "            advantage = returns - self.value_net(states_tensor).squeeze(-1)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = nn.functional.mse_loss(self.value_net(states_tensor).squeeze(-1), returns)\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()\n",
    "\n",
    "# Example usage\n",
    "env = PortfolioEnvRealTime(asset_symbols=['AAPL', 'GOOGL', 'TSLA'], initial_cash=100000)\n",
    "agent = PPOAgent(input_dim=len(env.asset_symbols) + 2, output_dim=len(env.asset_symbols))\n",
    "\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    states, actions, rewards, dones, old_action_probs = [], [], [], [], []\n",
    "\n",
    "    while not done:\n",
    "        action, action_prob = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step([action])\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        old_action_probs.append(action_prob)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    agent.update(states, actions, rewards, dones, old_action_probs)\n",
    "    print(f\"Episode {episode+1}: Total Reward = {episode_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
